"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[884],{2660(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-4-vla/vla-intro","title":"Module 4: Vision-Language-Action (VLA)","description":"Understanding VLA models and multimodal AI for robotics.","source":"@site/docs/module-4-vla/vla-intro.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/vla-intro","permalink":"/docs/module-4-vla/vla-intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/vla-intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"id":"vla-intro","title":"Module 4: Vision-Language-Action (VLA)","description":"Understanding VLA models and multimodal AI for robotics."},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS VSLAM & Nav2","permalink":"/docs/module-3-ai-robot-brain/nav2-vslam"},"next":{"title":"Voice-to-Action & Cognitive Planning","permalink":"/docs/module-4-vla/voice-cognitive-planning"}}');var t=i(4848),s=i(8453);const r={sidebar_position:1,id:"vla-intro",title:"Module 4: Vision-Language-Action (VLA)",description:"Understanding VLA models and multimodal AI for robotics."},l="Module 4: Vision-Language-Action (VLA)",a={},d=[{value:"1. Learning Objectives",id:"1-learning-objectives",level:2},{value:"2. Core Explanation (Intermediate)",id:"2-core-explanation-intermediate",level:2},{value:"3. Beginner Simplification",id:"3-beginner-simplification",level:2},{value:"4. Advanced Deep-Dive",id:"4-advanced-deep-dive",level:2},{value:"Tokenization of Actions",id:"tokenization-of-actions",level:3},{value:"Co-Fine-Tuning",id:"co-fine-tuning",level:3},{value:"5. Code Examples",id:"5-code-examples",level:2},{value:"VLA Inference Pseudo-Code (Python-like)",id:"vla-inference-pseudo-code-python-like",level:3},{value:"6. Real-World Humanoid Robotics Context",id:"6-real-world-humanoid-robotics-context",level:2},{value:"7. Exercises",id:"7-exercises",level:2},{value:"8. Assessment Questions",id:"8-assessment-questions",level:2},{value:"9. RAG-Friendly Summaries",id:"9-rag-friendly-summaries",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"1-learning-objectives",children:"1. Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Define"})," VLA (Vision-Language-Action) models."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explain"})," how Transformers are applied to robotic control."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Understand"})," the difference between LLMs (Language Models) and VLAs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement"})," a pseudo-code inference loop for a VLA."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"2-core-explanation-intermediate",children:"2. Core Explanation (Intermediate)"}),"\n",(0,t.jsxs)(n.p,{children:["Traditional robotics uses a pipeline: Perception -> Planning -> Control.\n",(0,t.jsx)(n.strong,{children:"VLA (Vision-Language-Action)"})," models replace this pipeline with a single end-to-end neural network."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),': The robot "sees" an image (e.g., a messy table).']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),': The user gives a command (e.g., "Put the apple in the bowl").']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": The model directly outputs the joint velocities or end-effector pose to perform the task."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["VLAs are built on ",(0,t.jsx)(n.strong,{children:"Transformers"})," (like GPT-4), but instead of just outputting text, they are fine-tuned to output ",(0,t.jsx)(n.em,{children:"robot actions"}),". Examples include Google's ",(0,t.jsx)(n.strong,{children:"RT-2"})," (Robotic Transformer 2)."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"3-beginner-simplification",children:"3. Beginner Simplification"}),"\n",(0,t.jsxs)(n.p,{children:["Think of a ",(0,t.jsx)(n.strong,{children:"Super-Smart Assistant"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Old Way"}),': You tell a robot "Pick up the cup." The robot has to:']}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Find the cup (Object Detection)."}),"\n",(0,t.jsx)(n.li,{children:"Calculate a path (Motion Planning)."}),"\n",(0,t.jsx)(n.li,{children:"Move the arm (Control).\nIf any step fails, the whole thing fails."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"VLA Way"}),': You show the robot a picture of the cup and say "Pick it up."']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The robot's brain (VLA) instantly knows \"Okay, to do that, I need to move my hand ",(0,t.jsx)(n.em,{children:"here"}),' and close my fingers."']}),"\n",(0,t.jsx)(n.li,{children:"It's like intuition. It learns from seeing millions of examples, just like a human."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"4-advanced-deep-dive",children:"4. Advanced Deep-Dive"}),"\n",(0,t.jsx)(n.h3,{id:"tokenization-of-actions",children:"Tokenization of Actions"}),"\n",(0,t.jsx)(n.p,{children:'Large Language Models (LLMs) work on "tokens" (parts of words). To make an LLM control a robot, we must "tokenize" actions.'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Discretization"}),": We divide the continuous space of robot movement (e.g., moving the arm 10cm) into discrete bins (e.g., integers 0 to 255)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Input"}),": Image embeddings (from a ViT) + Text embeddings."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Output"}),": A sequence of tokens representing the gripper's x, y, z, roll, pitch, yaw, and opening width."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"co-fine-tuning",children:"Co-Fine-Tuning"}),"\n",(0,t.jsx)(n.p,{children:"RT-2 is trained on both internet data (web text and images) and robot data (trajectories of arms moving)."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Internet Data"}),' gives it "Common Sense" (knowing what a "Superman toy" looks like, even if it\'s never picked one up).']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robot Data"}),' gives it "Physical Skills" (knowing how to grasp).\nThis allows the robot to perform tasks it has never been explicitly trained on (Generalization).']}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"5-code-examples",children:"5. Code Examples"}),"\n",(0,t.jsx)(n.h3,{id:"vla-inference-pseudo-code-python-like",children:"VLA Inference Pseudo-Code (Python-like)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# This is a conceptual example of how a VLA is used.\n\nclass VLA_Model:\n    def predict_action(self, image, instruction):\n        # 1. Encode the image and text\n        img_tokens = self.vision_encoder(image)\n        text_tokens = self.tokenizer(instruction)\n        \n        # 2. Feed into the Transformer\n        input_sequence = img_tokens + text_tokens\n        output_tokens = self.transformer.generate(input_sequence)\n        \n        # 3. De-tokenize back to robot actions\n        action = self.detokenize(output_tokens)\n        return action # e.g., {x: 0.5, y: 0.2, z: 0.1, gripper: 1.0}\n\ndef main_loop(robot, camera, model):\n    instruction = "Pick up the red bull can."\n    \n    while True:\n        # Get current observation\n        image = camera.get_frame()\n        \n        # Ask VLA what to do\n        action = model.predict_action(image, instruction)\n        \n        # Execute action\n        robot.move_arm(action)\n        \n        if action.is_terminate():\n            break\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"6-real-world-humanoid-robotics-context",children:"6. Real-World Humanoid Robotics Context"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Figure 01 + OpenAI"}),':\nIn a famous demo, a human asks Figure 01, "Can I have something to eat?"']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The robot sees an apple on the table."}),"\n",(0,t.jsx)(n.li,{children:'Its VLA (powered by OpenAI) reasons: "The apple is the only edible item. The user wants to eat. Therefore, I should pick up the apple."'}),"\n",(0,t.jsx)(n.li,{children:"It then executes the pick-and-place action.\nThis semantic reasoning combined with low-level control is the promise of VLAs."}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"7-exercises",children:"7. Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Concept"}),": Draw a diagram showing the inputs and outputs of an RT-2 model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Research"}),': Look up the paper "RT-2: Vision-Language-Action Models" by Google DeepMind. Read the abstract.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Thought Experiment"}),': If you asked a VLA robot to "clean the spill," but there was no sponge, what should it do? (Ideally, reason that it needs to find a tool or ask for help).']}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"8-assessment-questions",children:"8. Assessment Questions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple Choice"}),': What does the "A" in VLA stand for?',"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"a) Algorithm"}),"\n",(0,t.jsx)(n.li,{children:"b) Action"}),"\n",(0,t.jsx)(n.li,{children:"c) Automation"}),"\n",(0,t.jsx)(n.li,{children:"d) Agent"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Short Answer"}),": How do we turn continuous robot movement into something a Transformer can understand? (Tokenization / Discretization)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explain"}),": Why does training on internet data help a robot pick up a specific object it has never seen before? (Generalization / Common Sense transfer)."]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"9-rag-friendly-summaries",children:"9. RAG-Friendly Summaries"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Chunk 1: VLA Definition"}),"\nVLA (Vision-Language-Action) models are multimodal neural networks that take visual and textual inputs and directly output robotic control actions. They unify perception, reasoning, and control into a single model."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Chunk 2: Action Tokenization"}),"\nTo use Transformers for robotics, continuous physical actions (like arm movements) are discretized into tokens. This allows the model to predict physical actions using the same mechanism it uses to predict the next word in a sentence."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Chunk 3: Generalization"}),"\nVLAs like RT-2 leverage massive datasets of web text and images to gain semantic understanding (common sense). This allows them to perform novel tasks with objects they haven't explicitly trained with, bridging the gap between high-level reasoning and low-level control."]}),"\n",(0,t.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function r(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);