"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[845],{1184(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>c,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"module-2-digital-twin/sensor-simulation","title":"Sensor Simulation: LiDAR, Depth Camera, IMU","description":"Configuring simulated sensors and ROS 2 bridges for humanoids.","source":"@site/docs/module-2-digital-twin/sensor-simulation.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/sensor-simulation","permalink":"/docs/module-2-digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2-digital-twin/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"sensor-simulation","title":"Sensor Simulation: LiDAR, Depth Camera, IMU","description":"Configuring simulated sensors and ROS 2 bridges for humanoids.","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"URDF & SDF for Humanoid Robots","permalink":"/docs/module-2-digital-twin/urdf-sdf-humanoids"},"next":{"title":"Unity for High-Fidelity HRI","permalink":"/docs/module-2-digital-twin/unity-hri"}}');var s=i(4848),r=i(8453);const o={id:"sensor-simulation",title:"Sensor Simulation: LiDAR, Depth Camera, IMU",description:"Configuring simulated sensors and ROS 2 bridges for humanoids.",sidebar_position:5},t="Sensor Simulation: LiDAR, Depth Camera, IMU",l={},d=[{value:"Beginner",id:"beginner",level:2},{value:"Intermediate",id:"intermediate",level:2},{value:"Advanced",id:"advanced",level:2},{value:"Gazebo SDF Configuration Examples",id:"gazebo-sdf-configuration-examples",level:2},{value:"Humanoid Context",id:"humanoid-context",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Assessments",id:"assessments",level:2},{value:"RAG Summary Chunks",id:"rag-summary-chunks",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"sensor-simulation-lidar-depth-camera-imu",children:"Sensor Simulation: LiDAR, Depth Camera, IMU"})}),"\n",(0,s.jsx)(n.h2,{id:"beginner",children:"Beginner"}),"\n",(0,s.jsx)(n.p,{children:"Simulated sensors mimic real devices. LiDAR measures distances, depth cameras provide 3D images, and IMUs report orientation and acceleration. These help humanoids see and balance in the virtual world."}),"\n",(0,s.jsx)(n.h2,{id:"intermediate",children:"Intermediate"}),"\n",(0,s.jsx)(n.p,{children:"Attach sensor plugins to models and publish ROS 2 topics. Tune update rates and resolution. Use TF frames to align sensors with robot links. Test perception nodes against simulated topics."}),"\n",(0,s.jsx)(n.h2,{id:"advanced",children:"Advanced"}),"\n",(0,s.jsx)(n.p,{children:"Optimize sensor rates to balance fidelity and performance. Synchronize sensors with message filters. Calibrate noise models for robust algorithms. Use headless rendering when possible and ensure frame consistency."}),"\n",(0,s.jsx)(n.h2,{id:"gazebo-sdf-configuration-examples",children:"Gazebo SDF Configuration Examples"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="lidar" type="ray">\n  <update_rate>15</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1024</samples>\n        <min_angle>-1.57</min_angle>\n        <max_angle>1.57</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>20.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="gazebo_ros_laser" filename="libgazebo_ros_laser.so">\n    <topicName>/scan</topicName>\n    <frameName>head_link</frameName>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_cam" type="depth">\n  <update_rate>30</update_rate>\n  <camera>\n    <horizontal_fov>1.0</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10.0</far>\n    </clip>\n  </camera>\n  <plugin name="gazebo_ros_depth_camera" filename="libgazebo_ros_depth_camera.so">\n    <imageTopicName>/camera/color/image_raw</imageTopicName>\n    <depthImageTopicName>/camera/depth/image_raw</depthImageTopicName>\n    <cameraInfoTopicName>/camera/color/camera_info</cameraInfoTopicName>\n    <frameName>head_link</frameName>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<sensor name="imu" type="imu">\n  <update_rate>200</update_rate>\n  <plugin name="gazebo_ros_imu" filename="libgazebo_ros_imu.so">\n    <topicName>/imu/data</topicName>\n    <frameName>base_link</frameName>\n  </plugin>\n</sensor>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"humanoid-context",children:"Humanoid Context"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Head-mounted depth camera supports grasp and obstacle detection."}),"\n",(0,s.jsx)(n.li,{children:"IMU provides orientation for balance and fall detection."}),"\n",(0,s.jsx)(n.li,{children:"LiDAR assists in mapping rooms and navigation for humanoids."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Attach sensors to a humanoid head link and verify topics."}),"\n",(0,s.jsx)(n.li,{children:"Subscribe to IMU and compute roll, pitch, yaw."}),"\n",(0,s.jsx)(n.li,{children:"Synchronize depth and RGB streams and test a perception node."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessments",children:"Assessments"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Multiple Choice: Which sensor reports orientation and acceleration?"}),"\n",(0,s.jsx)(n.li,{children:"Short Answer: Why synchronize RGB and depth streams?"}),"\n",(0,s.jsx)(n.li,{children:"Scenario: Perception lags. Which sensor parameters should be tuned?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"rag-summary-chunks",children:"RAG Summary Chunks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Chunk 1: Simulated LiDAR, depth, and IMU publish ROS 2 topics via plugins."}),"\n",(0,s.jsx)(n.li,{children:"Chunk 2: Frame alignment and update rates are critical for humanoid tasks."}),"\n",(0,s.jsx)(n.li,{children:"Chunk 3: Synchronization and noise models improve robustness."}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453(e,n,i){i.d(n,{R:()=>o,x:()=>t});var a=i(6540);const s={},r=a.createContext(s);function o(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);