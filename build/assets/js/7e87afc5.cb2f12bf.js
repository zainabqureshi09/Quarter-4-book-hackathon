"use strict";(self.webpackChunkphysical_ai_robotics_textbook=self.webpackChunkphysical_ai_robotics_textbook||[]).push([[13],{4256(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/voice-cognitive-planning","title":"Voice-to-Action & Cognitive Planning","description":"Using OpenAI Whisper and LLMs to translate natural language into robot actions.","source":"@site/docs/module-4-vla/voice-cognitive-planning.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/voice-cognitive-planning","permalink":"/docs/module-4-vla/voice-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/voice-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"id":"voice-cognitive-planning","title":"Voice-to-Action & Cognitive Planning","description":"Using OpenAI Whisper and LLMs to translate natural language into robot actions."},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/docs/module-4-vla/vla-intro"},"next":{"title":"Capstone Project: Pick-and-Place with Humanoid","permalink":"/docs/capstone-project/capstone-intro"}}');var o=i(4848),s=i(8453);const r={sidebar_position:2,id:"voice-cognitive-planning",title:"Voice-to-Action & Cognitive Planning",description:"Using OpenAI Whisper and LLMs to translate natural language into robot actions."},l="Voice-to-Action & Cognitive Planning",a={},c=[{value:"1. Learning Objectives",id:"1-learning-objectives",level:2},{value:"2. Core Explanation (Intermediate)",id:"2-core-explanation-intermediate",level:2},{value:"Voice-to-Action (Whisper)",id:"voice-to-action-whisper",level:3},{value:"Cognitive Planning (LLMs)",id:"cognitive-planning-llms",level:3},{value:"3. Beginner Simplification",id:"3-beginner-simplification",level:2},{value:"4. Advanced Deep-Dive",id:"4-advanced-deep-dive",level:2},{value:"JSON Enforcement",id:"json-enforcement",level:3},{value:"Closed-Loop Planning",id:"closed-loop-planning",level:3},{value:"5. Code Examples",id:"5-code-examples",level:2},{value:"Voice-to-Plan Pipeline (Python)",id:"voice-to-plan-pipeline-python",level:3},{value:"6. Real-World Context",id:"6-real-world-context",level:2},{value:"7. Exercises",id:"7-exercises",level:2},{value:"8. Assessment Questions",id:"8-assessment-questions",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-to-action--cognitive-planning",children:"Voice-to-Action & Cognitive Planning"})}),"\n",(0,o.jsx)(n.h2,{id:"1-learning-objectives",children:"1. Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integrate"})," OpenAI Whisper for robust speech-to-text transcription."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Design"}),' a "Cognitive Planner" using an LLM (like GPT-4) to decompose tasks.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Translate"}),' natural language ("Clean the room") into structured ROS 2 actions.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Implement"})," a pipeline: Voice -> Text -> Plan -> Execution."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"2-core-explanation-intermediate",children:"2. Core Explanation (Intermediate)"}),"\n",(0,o.jsx)(n.h3,{id:"voice-to-action-whisper",children:"Voice-to-Action (Whisper)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"OpenAI Whisper"})," is a state-of-the-art automatic speech recognition (ASR) system. It is robust to accents and background noise, making it ideal for real-world robotics."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Input"}),": Audio waveform from the robot's microphone."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Output"}),': Text string (e.g., "Go to the kitchen and find the apple").']}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"cognitive-planning-llms",children:"Cognitive Planning (LLMs)"}),"\n",(0,o.jsxs)(n.p,{children:['Robots understand code (Move to X, Y, Z), not vague commands ("Clean up"). We use ',(0,o.jsx)(n.strong,{children:"Large Language Models (LLMs)"})," as a translation layer."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prompt Engineering"}),": We give the LLM a list of available robot skills (e.g., ",(0,o.jsx)(n.code,{children:"pick()"}),", ",(0,o.jsx)(n.code,{children:"place()"}),", ",(0,o.jsx)(n.code,{children:"navigate()"}),") and ask it to break down the user's request into a sequence of these skills."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Chain of Thought"}),': The LLM reasons: "To clean the room, I first need to look for trash. If I see a cup, I should pick it up..."']}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"3-beginner-simplification",children:"3. Beginner Simplification"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"The Translator"}),':\r\nImagine the robot speaks "Robot Code" and you speak "English."']}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Whisper"}),": Listens to your voice and writes it down as text."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"The Planner (LLM)"}),': Reads the text ("Make me a sandwich") and writes a recipe in Robot Code:',"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"walk_to(kitchen)"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"find(bread)"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"pick(bread)"})}),"\n",(0,o.jsx)(n.li,{children:"..."}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"The Robot"}),": Executes the recipe step-by-step."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"4-advanced-deep-dive",children:"4. Advanced Deep-Dive"}),"\n",(0,o.jsx)(n.h3,{id:"json-enforcement",children:"JSON Enforcement"}),"\n",(0,o.jsxs)(n.p,{children:["To ensure the LLM outputs valid code that the robot can parse, we use ",(0,o.jsx)(n.strong,{children:"JSON Schema enforcement"})," or libraries like ",(0,o.jsx)(n.code,{children:"guidance"}),"."]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prompt"}),': "You are a robot planner. Output ONLY a JSON list of actions."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Response"}),": ",(0,o.jsx)(n.code,{children:'[{"action": "navigate", "target": "kitchen"}, {"action": "pick", "object": "apple"}]'})]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"closed-loop-planning",children:"Closed-Loop Planning"}),"\n",(0,o.jsxs)(n.p,{children:["A static plan might fail (e.g., the apple isn't in the kitchen). ",(0,o.jsx)(n.strong,{children:"Closed-Loop Planning"}),' feeds the robot\'s feedback ("Object not found") back into the LLM, asking it to replan ("Okay, try searching the dining room instead").']}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"5-code-examples",children:"5. Code Examples"}),"\n",(0,o.jsx)(n.h3,{id:"voice-to-plan-pipeline-python",children:"Voice-to-Plan Pipeline (Python)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\r\nimport whisper\r\n\r\n# 1. Transcribe Audio\r\nmodel = whisper.load_model("base")\r\nresult = model.transcribe("audio_command.wav")\r\nuser_text = result["text"] # "Get me a soda."\r\n\r\n# 2. Cognitive Planning (LLM)\r\nsystem_prompt = """\r\nYou are a helper robot. Available functions:\r\n- navigate(location)\r\n- pick(object)\r\n- place(location)\r\nConvert the user request into a list of function calls.\r\n"""\r\n\r\nresponse = openai.ChatCompletion.create(\r\n    model="gpt-4",\r\n    messages=[\r\n        {"role": "system", "content": system_prompt},\r\n        {"role": "user", "content": user_text}\r\n    ]\r\n)\r\n\r\nplan = response.choices[0].message.content\r\nprint(plan)\r\n# Output:\r\n# navigate("kitchen")\r\n# pick("soda")\r\n# navigate("user")\r\n# place("user_table")\n'})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"6-real-world-context",children:"6. Real-World Context"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Google PaLM-E"}),':\r\nGoogle\'s PaLM-E model connects language directly to robot perception and control, allowing it to solve long-horizon tasks like "Bring me the rice chips from the drawer" by reasoning about the environment state visually.']}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"7-exercises",children:"7. Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Whisper"}),": Record a command and transcribe it using the Whisper Python API."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prompting"}),': Write a system prompt that forces an LLM to output valid JSON actions for a "Tidying Robot."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integration"}),": Create a script that takes a text string and calls dummy Python functions (",(0,o.jsx)(n.code,{children:'print("Moving to...")'}),") based on the LLM's output."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"8-assessment-questions",children:"8. Assessment Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Explain"}),": Why do we need an LLM for robotics instead of just hard-coding commands? (Flexibility to understand natural language and novel variations)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Debug"}),': The LLM outputs "Grab the thingy." Why does the robot fail? (The robot needs specific object IDs, not vague nouns. The prompt must enforce strict vocabulary).']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Design"}),': How would you handle a user command that is dangerous ("Jump off the cliff")? (Add a "Safety Filter" layer before execution).']}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>l});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);