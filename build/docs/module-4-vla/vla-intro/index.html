<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-4-vla/vla-intro" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Module 4: Vision-Language-Action (VLA) | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/docs/module-4-vla/vla-intro"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Module 4: Vision-Language-Action (VLA) | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Understanding VLA models and multimodal AI for robotics."><meta data-rh="true" property="og:description" content="Understanding VLA models and multimodal AI for robotics."><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/docs/module-4-vla/vla-intro"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/module-4-vla/vla-intro" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/module-4-vla/vla-intro" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Module 4: Vision-Language-Action (VLA)","item":"https://your-docusaurus-site.example.com/docs/module-4-vla/vla-intro"}]}</script><link rel="stylesheet" href="/assets/css/styles.f6412f5c.css">
<script src="/assets/js/runtime~main.02b5b821.js" defer="defer"></script>
<script src="/assets/js/main.14d4abe3.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"dark"),document.documentElement.setAttribute("data-theme-choice",t||"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Textbook</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro"><span title="Introduction to Physical AI &amp; Humanoid Robotics" class="linkLabel_WmDU">Introduction to Physical AI &amp; Humanoid Robotics</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/module-1-ros2/ros2-intro"><span title="Module 1: ROS 2" class="categoryLinkLabel_W154">Module 1: ROS 2</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/module-2-digital-twin/digital-twin-intro"><span title="Module 2: Digital Twin" class="categoryLinkLabel_W154">Module 2: Digital Twin</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/module-3-ai-robot-brain/isaac-intro"><span title="Module 3: AI-Robot Brain" class="categoryLinkLabel_W154">Module 3: AI-Robot Brain</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/docs/module-4-vla/vla-intro"><span title="Module 4: VLA" class="categoryLinkLabel_W154">Module 4: VLA</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/module-4-vla/vla-intro"><span title="Module 4: Vision-Language-Action (VLA)" class="linkLabel_WmDU">Module 4: Vision-Language-Action (VLA)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/module-4-vla/voice-cognitive-planning"><span title="Voice-to-Action &amp; Cognitive Planning" class="linkLabel_WmDU">Voice-to-Action &amp; Cognitive Planning</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/docs/capstone-project/capstone-intro"><span title="Capstone Project" class="categoryLinkLabel_W154">Capstone Project</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 4: VLA</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Module 4: Vision-Language-Action (VLA)</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Module 4: Vision-Language-Action (VLA)</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-learning-objectives">1. Learning Objectives<a href="#1-learning-objectives" class="hash-link" aria-label="Direct link to 1. Learning Objectives" title="Direct link to 1. Learning Objectives" translate="no">​</a></h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class=""><strong>Define</strong> VLA (Vision-Language-Action) models.</li>
<li class=""><strong>Explain</strong> how Transformers are applied to robotic control.</li>
<li class=""><strong>Understand</strong> the difference between LLMs (Language Models) and VLAs.</li>
<li class=""><strong>Implement</strong> a pseudo-code inference loop for a VLA.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-core-explanation-intermediate">2. Core Explanation (Intermediate)<a href="#2-core-explanation-intermediate" class="hash-link" aria-label="Direct link to 2. Core Explanation (Intermediate)" title="Direct link to 2. Core Explanation (Intermediate)" translate="no">​</a></h2>
<p>Traditional robotics uses a pipeline: Perception -&gt; Planning -&gt; Control.
<strong>VLA (Vision-Language-Action)</strong> models replace this pipeline with a single end-to-end neural network.</p>
<ul>
<li class=""><strong>Vision</strong>: The robot &quot;sees&quot; an image (e.g., a messy table).</li>
<li class=""><strong>Language</strong>: The user gives a command (e.g., &quot;Put the apple in the bowl&quot;).</li>
<li class=""><strong>Action</strong>: The model directly outputs the joint velocities or end-effector pose to perform the task.</li>
</ul>
<p>VLAs are built on <strong>Transformers</strong> (like GPT-4), but instead of just outputting text, they are fine-tuned to output <em>robot actions</em>. Examples include Google&#x27;s <strong>RT-2</strong> (Robotic Transformer 2).</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-beginner-simplification">3. Beginner Simplification<a href="#3-beginner-simplification" class="hash-link" aria-label="Direct link to 3. Beginner Simplification" title="Direct link to 3. Beginner Simplification" translate="no">​</a></h2>
<p>Think of a <strong>Super-Smart Assistant</strong>:</p>
<ul>
<li class="">
<p><strong>Old Way</strong>: You tell a robot &quot;Pick up the cup.&quot; The robot has to:</p>
<ol>
<li class="">Find the cup (Object Detection).</li>
<li class="">Calculate a path (Motion Planning).</li>
<li class="">Move the arm (Control).
If any step fails, the whole thing fails.</li>
</ol>
</li>
<li class="">
<p><strong>VLA Way</strong>: You show the robot a picture of the cup and say &quot;Pick it up.&quot;</p>
<ul>
<li class="">The robot&#x27;s brain (VLA) instantly knows &quot;Okay, to do that, I need to move my hand <em>here</em> and close my fingers.&quot;</li>
<li class="">It&#x27;s like intuition. It learns from seeing millions of examples, just like a human.</li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="4-advanced-deep-dive">4. Advanced Deep-Dive<a href="#4-advanced-deep-dive" class="hash-link" aria-label="Direct link to 4. Advanced Deep-Dive" title="Direct link to 4. Advanced Deep-Dive" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="tokenization-of-actions">Tokenization of Actions<a href="#tokenization-of-actions" class="hash-link" aria-label="Direct link to Tokenization of Actions" title="Direct link to Tokenization of Actions" translate="no">​</a></h3>
<p>Large Language Models (LLMs) work on &quot;tokens&quot; (parts of words). To make an LLM control a robot, we must &quot;tokenize&quot; actions.</p>
<ul>
<li class=""><strong>Discretization</strong>: We divide the continuous space of robot movement (e.g., moving the arm 10cm) into discrete bins (e.g., integers 0 to 255).</li>
<li class=""><strong>Input</strong>: Image embeddings (from a ViT) + Text embeddings.</li>
<li class=""><strong>Output</strong>: A sequence of tokens representing the gripper&#x27;s x, y, z, roll, pitch, yaw, and opening width.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="co-fine-tuning">Co-Fine-Tuning<a href="#co-fine-tuning" class="hash-link" aria-label="Direct link to Co-Fine-Tuning" title="Direct link to Co-Fine-Tuning" translate="no">​</a></h3>
<p>RT-2 is trained on both internet data (web text and images) and robot data (trajectories of arms moving).</p>
<ul>
<li class=""><strong>Internet Data</strong> gives it &quot;Common Sense&quot; (knowing what a &quot;Superman toy&quot; looks like, even if it&#x27;s never picked one up).</li>
<li class=""><strong>Robot Data</strong> gives it &quot;Physical Skills&quot; (knowing how to grasp).
This allows the robot to perform tasks it has never been explicitly trained on (Generalization).</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="5-code-examples">5. Code Examples<a href="#5-code-examples" class="hash-link" aria-label="Direct link to 5. Code Examples" title="Direct link to 5. Code Examples" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="vla-inference-pseudo-code-python-like">VLA Inference Pseudo-Code (Python-like)<a href="#vla-inference-pseudo-code-python-like" class="hash-link" aria-label="Direct link to VLA Inference Pseudo-Code (Python-like)" title="Direct link to VLA Inference Pseudo-Code (Python-like)" translate="no">​</a></h3>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># This is a conceptual example of how a VLA is used.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">class</span><span class="token plain"> </span><span class="token class-name">VLA_Model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">predict_action</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> image</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> instruction</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># 1. Encode the image and text</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        img_tokens </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">vision_encoder</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">image</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        text_tokens </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">tokenizer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">instruction</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># 2. Feed into the Transformer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        input_sequence </span><span class="token operator">=</span><span class="token plain"> img_tokens </span><span class="token operator">+</span><span class="token plain"> text_tokens</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        output_tokens </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">transformer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">generate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">input_sequence</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># 3. De-tokenize back to robot actions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        action </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">detokenize</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">output_tokens</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">return</span><span class="token plain"> action </span><span class="token comment" style="color:rgb(98, 114, 164)"># e.g., {x: 0.5, y: 0.2, z: 0.1, gripper: 1.0}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(80, 250, 123)">main_loop</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">robot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> camera</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    instruction </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Pick up the red bull can.&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">while</span><span class="token plain"> </span><span class="token boolean">True</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Get current observation</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        image </span><span class="token operator">=</span><span class="token plain"> camera</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">get_frame</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Ask VLA what to do</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        action </span><span class="token operator">=</span><span class="token plain"> model</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">predict_action</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">image</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> instruction</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token comment" style="color:rgb(98, 114, 164)"># Execute action</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        robot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">move_arm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">action</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> action</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">is_terminate</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">break</span><br></span></code></pre></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="6-real-world-humanoid-robotics-context">6. Real-World Humanoid Robotics Context<a href="#6-real-world-humanoid-robotics-context" class="hash-link" aria-label="Direct link to 6. Real-World Humanoid Robotics Context" title="Direct link to 6. Real-World Humanoid Robotics Context" translate="no">​</a></h2>
<p><strong>Figure 01 + OpenAI</strong>:
In a famous demo, a human asks Figure 01, &quot;Can I have something to eat?&quot;</p>
<ul>
<li class="">The robot sees an apple on the table.</li>
<li class="">Its VLA (powered by OpenAI) reasons: &quot;The apple is the only edible item. The user wants to eat. Therefore, I should pick up the apple.&quot;</li>
<li class="">It then executes the pick-and-place action.
This semantic reasoning combined with low-level control is the promise of VLAs.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="7-exercises">7. Exercises<a href="#7-exercises" class="hash-link" aria-label="Direct link to 7. Exercises" title="Direct link to 7. Exercises" translate="no">​</a></h2>
<ol>
<li class=""><strong>Concept</strong>: Draw a diagram showing the inputs and outputs of an RT-2 model.</li>
<li class=""><strong>Research</strong>: Look up the paper &quot;RT-2: Vision-Language-Action Models&quot; by Google DeepMind. Read the abstract.</li>
<li class=""><strong>Thought Experiment</strong>: If you asked a VLA robot to &quot;clean the spill,&quot; but there was no sponge, what should it do? (Ideally, reason that it needs to find a tool or ask for help).</li>
</ol>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="8-assessment-questions">8. Assessment Questions<a href="#8-assessment-questions" class="hash-link" aria-label="Direct link to 8. Assessment Questions" title="Direct link to 8. Assessment Questions" translate="no">​</a></h2>
<ol>
<li class=""><strong>Multiple Choice</strong>: What does the &quot;A&quot; in VLA stand for?<!-- -->
<ul>
<li class="">a) Algorithm</li>
<li class="">b) Action</li>
<li class="">c) Automation</li>
<li class="">d) Agent</li>
</ul>
</li>
<li class=""><strong>Short Answer</strong>: How do we turn continuous robot movement into something a Transformer can understand? (Tokenization / Discretization).</li>
<li class=""><strong>Explain</strong>: Why does training on internet data help a robot pick up a specific object it has never seen before? (Generalization / Common Sense transfer).</li>
</ol>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="9-rag-friendly-summaries">9. RAG-Friendly Summaries<a href="#9-rag-friendly-summaries" class="hash-link" aria-label="Direct link to 9. RAG-Friendly Summaries" title="Direct link to 9. RAG-Friendly Summaries" translate="no">​</a></h2>
<p><strong>Chunk 1: VLA Definition</strong>
VLA (Vision-Language-Action) models are multimodal neural networks that take visual and textual inputs and directly output robotic control actions. They unify perception, reasoning, and control into a single model.</p>
<p><strong>Chunk 2: Action Tokenization</strong>
To use Transformers for robotics, continuous physical actions (like arm movements) are discretized into tokens. This allows the model to predict physical actions using the same mechanism it uses to predict the next word in a sentence.</p>
<p><strong>Chunk 3: Generalization</strong>
VLAs like RT-2 leverage massive datasets of web text and images to gain semantic understanding (common sense). This allows them to perform novel tasks with objects they haven&#x27;t explicitly trained with, bridging the gap between high-level reasoning and low-level control.</p>
<hr></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4-vla/vla-intro.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/module-3-ai-robot-brain/nav2-vslam"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Isaac ROS VSLAM &amp; Nav2</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/module-4-vla/voice-cognitive-planning"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Voice-to-Action &amp; Cognitive Planning</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-learning-objectives" class="table-of-contents__link toc-highlight">1. Learning Objectives</a></li><li><a href="#2-core-explanation-intermediate" class="table-of-contents__link toc-highlight">2. Core Explanation (Intermediate)</a></li><li><a href="#3-beginner-simplification" class="table-of-contents__link toc-highlight">3. Beginner Simplification</a></li><li><a href="#4-advanced-deep-dive" class="table-of-contents__link toc-highlight">4. Advanced Deep-Dive</a><ul><li><a href="#tokenization-of-actions" class="table-of-contents__link toc-highlight">Tokenization of Actions</a></li><li><a href="#co-fine-tuning" class="table-of-contents__link toc-highlight">Co-Fine-Tuning</a></li></ul></li><li><a href="#5-code-examples" class="table-of-contents__link toc-highlight">5. Code Examples</a><ul><li><a href="#vla-inference-pseudo-code-python-like" class="table-of-contents__link toc-highlight">VLA Inference Pseudo-Code (Python-like)</a></li></ul></li><li><a href="#6-real-world-humanoid-robotics-context" class="table-of-contents__link toc-highlight">6. Real-World Humanoid Robotics Context</a></li><li><a href="#7-exercises" class="table-of-contents__link toc-highlight">7. Exercises</a></li><li><a href="#8-assessment-questions" class="table-of-contents__link toc-highlight">8. Assessment Questions</a></li><li><a href="#9-rag-friendly-summaries" class="table-of-contents__link toc-highlight">9. RAG-Friendly Summaries</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Textbook</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics Textbook. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>